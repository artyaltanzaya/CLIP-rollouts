{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arty/anaconda3/envs/clip/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "from IPython.core.display import HTML\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "microwave = './examples/micro.mp4'\n",
    "kettle = './examples/kettle.mp4'\n",
    "light = './examples/light_switch.mp4'\n",
    "slide_cabinet = './examples/slide_cabinet.mp4'\n",
    "hinge_cabinet = './examples/hinge_cabinet.mp4'\n",
    "bottom_burner = './examples/bottom_burner.mp4'\n",
    "top_burner = './examples/top_burner.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tasks = [microwave, kettle, light, slide_cabinet, hinge_cabinet, bottom_burner, top_burner]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_task(task_name, prompt, plot=True, show_frames=False, friend_frames=1):\n",
    "    frames = []\n",
    "    video_cv2 = cv2.VideoCapture(task_name)\n",
    "    frame_cv2 = video_cv2.get(cv2.CAP_PROP_FPS)\n",
    "    current_frame = 0\n",
    "    while video_cv2.isOpened():\n",
    "        ret, frame = video_cv2.read()\n",
    "        if ret == True:\n",
    "            frames.append(Image.fromarray(frame[:, :, ::-1]))\n",
    "        else:\n",
    "            break\n",
    "        current_frame += 1\n",
    "        video_cv2.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "    \n",
    "    print(f\"Frames extracted: {len(frames)}\")\n",
    "\n",
    "    batch_size = 24\n",
    "    video_batch = math.ceil(len(frames) / batch_size)\n",
    "    image_features = torch.empty([0, 512], dtype=torch.float16).to(device)\n",
    "\n",
    "    for i in range(video_batch):\n",
    "        print(f\"Processing batch {i+1}/{video_batch}\")\n",
    "        frame_batch = frames[i*batch_size : (i+1)*batch_size]\n",
    "        preprocess_batch = torch.stack([preprocess(frame) for frame in frame_batch]).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_features = model.encode_image(preprocess_batch)\n",
    "            batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "        image_features = torch.cat((image_features, batch_features))\n",
    "\n",
    "    print(f\"Features: {image_features.shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(clip.tokenize(prompt).to(device))\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    top_frame, index_frame  = similarity.topk(friend_frames, dim=0)\n",
    "    if plot:\n",
    "        fig = px.imshow(similarity.cpu().numpy(), aspect='auto', color_continuous_scale='cividis')\n",
    "        fig.update_layout(coloraxis_showscale=True)\n",
    "        fig.update_xaxes(showticklabels=True)\n",
    "        fig.update_yaxes(showticklabels=True)\n",
    "        # fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "        fig.show()\n",
    "        plt.plot(similarity.cpu().numpy())\n",
    "        print()\n",
    "    \n",
    "    if show_frames:\n",
    "        for i in index_frame:\n",
    "            display(frames[i])\n",
    "            seconds = round(i.cpu().numpy()[0] * 1 / frame_cv2)\n",
    "            display(HTML(f\"{str(datetime.timedelta(seconds=seconds))} (<a target=\\\"_blank\\\" &t={seconds}\\\">link</a>)\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_task(task_name, prompt, plot=True, show_frames=False, friend_frames=1):\n",
    "    frames = []\n",
    "    video_cv2 = cv2.VideoCapture(task_name)\n",
    "    frame_cv2 = video_cv2.get(cv2.CAP_PROP_FPS)\n",
    "    current_frame = 0\n",
    "    while video_cv2.isOpened():\n",
    "        ret, frame = video_cv2.read()\n",
    "        if ret == True:\n",
    "            frames.append(Image.fromarray(frame[:, :, ::-1]))\n",
    "        else:\n",
    "            break\n",
    "        current_frame += 1\n",
    "        video_cv2.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "    \n",
    "    print(f\"Frames extracted: {len(frames)}\")\n",
    "\n",
    "    batch_size = 24\n",
    "    video_batch = math.ceil(len(frames) / batch_size)\n",
    "    image_features = torch.empty([0, 512], dtype=torch.float16).to(device)\n",
    "\n",
    "    for i in range(video_batch):\n",
    "        print(f\"Processing batch {i+1}/{video_batch}\")\n",
    "        frame_batch = frames[i*batch_size : (i+1)*batch_size]\n",
    "        preprocess_batch = torch.stack([preprocess(frame) for frame in frame_batch]).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_features = model.encode_image(preprocess_batch)\n",
    "            batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "        image_features = torch.cat((image_features, batch_features))\n",
    "\n",
    "    print(f\"Features: {image_features.shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(clip.tokenize(prompt).to(device))\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    top_frame, index_frame  = similarity.topk(friend_frames, dim=0)\n",
    "    if plot:\n",
    "        fig = px.imshow(similarity.cpu().numpy(), aspect='auto', color_continuous_scale='cividis')\n",
    "        fig.update_layout(coloraxis_showscale=True)\n",
    "        fig.update_xaxes(showticklabels=True)\n",
    "        fig.update_yaxes(showticklabels=True)\n",
    "        # fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "        fig.show()\n",
    "        plt.plot(similarity.cpu().numpy())\n",
    "        print()\n",
    "    \n",
    "    if show_frames:\n",
    "        for i in index_frame:\n",
    "            display(frames[i])\n",
    "            seconds = round(i.cpu().numpy()[0] * 1 / frame_cv2)\n",
    "            display(HTML(f\"{str(datetime.timedelta(seconds=seconds))} (<a target=\\\"_blank\\\" &t={seconds}\\\">link</a>)\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted: 70\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "video_cv2 = cv2.VideoCapture(kettle)\n",
    "frame_cv2 = video_cv2.get(cv2.CAP_PROP_FPS)\n",
    "current_frame = 0\n",
    "while video_cv2.isOpened():\n",
    "    ret, frame = video_cv2.read()\n",
    "    if ret == True:\n",
    "        frames.append(Image.fromarray(frame[:, :, ::-1]))\n",
    "    else:\n",
    "        break\n",
    "    current_frame += 1\n",
    "    video_cv2.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "\n",
    "print(f\"Frames extracted: {len(frames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = torch.tensor(np.stack([preprocess(frame) for frame in frames])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features /= image_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ask_task(task_name=kettle, prompt=\"push the kettle\", plot=True, show_frames=True, friend_frames=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('clip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "434444bb51e3a98a1d5090486558a4c859b424045d95d88d24016b7c171e928d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
